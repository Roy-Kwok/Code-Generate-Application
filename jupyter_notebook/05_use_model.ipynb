{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dda763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da9392b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('./tokenizer')\n",
    "tokenizer.add_special_tokens({\n",
    "    \"bos_token\":\"<s>\",\n",
    "    \"pad_token\":\"<pad>\",\n",
    "    \"eos_token\":\"</s>\",\n",
    "    \"unk_token\":\"<unk>\",\n",
    "    \"mask_token\":\"<mask>\",    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f03ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('model_save').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50f1b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55335\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "full_dirs = []\n",
    "for dir_path, path_names, filenames in os.walk('./repos'):\n",
    "    for filename in filenames:\n",
    "        full_dir = os.path.join(dir_path, filename)\n",
    "        full_dirs.append(full_dir)\n",
    "\n",
    "print(len(full_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8105c6e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import pickle\n",
      "from enum import Enum\n",
      "from pathlib import Path\n",
      "from typing import Any, Callable, Union\n",
      "\n",
      "from .types import StrBytes\n",
      "\n",
      "\n",
      "class Protocol(str, Enum):\n",
      "    json = 'json'\n",
      "    pickle = 'pickle'\n",
      "\n",
      "\n",
      "def load_str_bytes(\n",
      "    b: StrBytes,\n",
      "    *,\n",
      "    content_type: str = None,\n",
      "    encoding: str = 'utf8',\n",
      "    proto: Protocol = None,\n",
      "    allow_pickle: bool = False,\n",
      "    json_loads: Callable[[str], Any] = json.loads,\n",
      ") -> Any:\n",
      "    if proto is None and content_type:\n",
      "        if content_type.endswith(('json', 'javascript')):\n",
      "            pass\n",
      "        elif allow_pickle and content_type.endswith('pickle'):\n",
      "            proto = Protocol.pickle\n",
      "        else:\n",
      "            raise TypeError(f'Unknown content-type: {content_type}')\n",
      "\n",
      "    proto = proto or Protocol.json\n",
      "\n",
      "    if proto == Protocol.json:\n",
      "        if isinstance(b, bytes):\n",
      "            b = b.decode(encoding)\n",
      "        return json_loads(b)\n",
      "    elif proto == Protocol.pickle:\n",
      "        if not allow_pickle:\n",
      "            raise RuntimeError('Trying to decode with pickle with allow_pickle=False')\n",
      "        bb = b if isinstance(b, bytes) else b.encode()\n",
      "        return pickle.loads(bb)\n",
      "    else:\n",
      "        raise TypeError(f'Unknown protocol: {proto}')\n",
      "\n",
      "\n",
      "def load_file(\n",
      "    path: Union[str, Path],\n",
      "    *,\n",
      "    content_type: str = None,\n",
      "    encoding: str = 'utf8',\n",
      "    proto: Protocol = None,\n",
      "    allow_pickle: bool = False,\n",
      "    json_loads: Callable[[str], Any] = json.loads,\n",
      ") -> Any:\n",
      "    path = Path(path)\n",
      "    b = path.read_bytes()\n",
      "    if content_type is None:\n",
      "        if path.suffix in ('.js', '.json'):\n",
      "            proto = Protocol.json\n",
      "        elif path.suffix == '.pkl':\n",
      "            proto = Protocol.pickle\n",
      "\n",
      "    return load_str_bytes(\n",
      "        b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle, json_loads=json_loads\n",
      "    )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "full_dir = full_dirs[27555]\n",
    "try:\n",
    "    fd = open(full_dir, 'r').read()\n",
    "    print(fd)\n",
    "except Exception as e:\n",
    "    print(str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636c4581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> import json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import os\n",
      "import sys\n",
      "import typing\n",
      "from typing import List, Optional\n",
      "\n",
      "if sys.version_info >= (3, 8):\n",
      "    from pip._vendor.packaging.version import (  # pragma: no cover\n",
      "else:\n",
      "    from typing import Literal  # type: ignore\n",
      "\n",
      "\n",
      "try:\n",
      "    import typing_extensions  # pragma: ignore\n",
      "except ImportError:\n",
      "    # pragma: ignore[str] = None\n",
      "    except ImportError:\n",
      "        from typing_extensions import Literal\n",
      "else:  # type: no cover  # type.\n",
      "\n",
      "    else:\n",
      "        pass\n",
      "    finally:\n",
      "\n",
      "        from pip._vendor_extensions import Any\n",
      "else\n",
      "    try:\n",
      "    except AttributeError:\n",
      "    return None\n",
      "        from None\n",
      "except AttributeError:  # pragma: nocover\n",
      "    return False\n",
      "    else\n",
      "    # type:  # type[str = None\n",
      "\n",
      "]\n",
      "    except type: no_type = type: ignore[str, Sequence[str\n",
      "    )\n",
      "\n",
      "except ImportError: ignore[str]\n",
      "    except KeyError\n",
      "    ):\n",
      "    return True\n",
      "    type: Union[str, Any\n",
      "    from_extensions.Any\n",
      "    pass\n",
      "        type: no = None\n",
      "]\n",
      "\n",
      "else: ignore[str = None  # type: bool\n",
      "    if type: no_extensions = False\n",
      "        # type: ignore_type = False\n",
      "<N\n",
      "   ...\n",
      "    except>    except ImportError:  # type_type = None\n",
      ")\n",
      "    finally\n",
      "    yield\n",
      "    \"\"\"\n",
      "    else  # type: Optional[str] = type: no_str\n",
      "except KeyError\n",
      "except type: ignore[0\n",
      "    ignore\n",
      "    elif  # type: nocover\n",
      "\n",
      ")\n",
      "except ImportError\n",
      "    import None\n",
      "else | None\n",
      "N>else: Union[str = Union[str]  # type: type: no_text = None<N\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    inp = input('>>> ')\n",
    "    input_ids = tokenizer.encode(inp, return_tensors='pt').to('cuda')\n",
    "    \n",
    "    beam_output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=512,\n",
    "        num_beams=10,\n",
    "        temperature=0.7,\n",
    "        no_repeat_ngram_size=5,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    \n",
    "    for beam in beam_output:\n",
    "        out = tokenizer.decode(beam)\n",
    "        fout = out.replace('<N>','\\n')\n",
    "        print(str(fout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b73338",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
